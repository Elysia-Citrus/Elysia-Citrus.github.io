---
title: 第三集：最大信息系数的推理
published: 2026-01-20
description: '关于MIC的实现、算法以及证明'
image: '/covers/cover3.png'
tags: [数学]
category: ''
draft: false 
lang: ''
---

本篇文章较为完整地复现了最大信息系数论文的算法.除了代码实现外，还包含了从互信息MI到MIC的完整数学推导及公平性证明。代码基于minepy1.2.6编写.

## 结构：

1.代码运行结果

2.MIC的数学原理

3.生成矩阵的近似算法 

4.不可或缺的过程优化

5.证明：MIC->0当且仅当X和Y统计独立

5.1证明：X和Y相关 => MIC > 0

5.2证明：对于统计独立的数据，MIC收敛于0

5.3证明：绝大多数无噪函数，MIC趋近1

5.4证明对于大多数由不同可谓曲线构成的有限并集，MIC趋近于1：

5.5抗噪能力

6.什么是更好的统计量


## 代码运行结果

![MIC论文_figure4](/illustrations/micshiyanjieguo1.png)

![MIC论文_figure4](/illustrations/micshiyanjieguo2.png)

![MIC论文_在基因对匹配上的计算](/illustrations/Reproduced_Figure5.png)

![MIC论文_在菌群数据集上的计算](/illustrations/Figure6_Top200.png )

## MIC. 最大信息系数

MIC的核心目标是衡量两个变量X和Y之间的关联强度，无论这种关联是线性还是非线性的. 它的数学基础建立在互信息MI之上，并通过离散化网络近似计算互信息以解决以往难以在连续变量上计算互信息的问题. 

*MINE. 基于最大信息的非参数探索. 利用计算MIC过程中生成的特征矩阵衍生出其它辅助性指标. 非参数估计，因此不用假设分布. *


互信息反映两个变量共享信息的度量. 也就是说知道随机变量X，随机变量Y的熵的减小. 互信息有定义式：

$$I(X; Y) = \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log \left( \frac{p(x, y)}{p(x)p(y)} \right). $$

该公式量化了知道X后，Y的不确定性减小了多少. 这也可以通过熵的等价形式来表达：

$$I(X; Y) = H(X) - H(X | Y) = H(Y) - H(Y | X)$$

可以从该公式中看出，互信息是对称的. 而且，互信息仅关注变量间的概率分布关系，而不依赖于变量的具体数值大小，这使得它自然得能够具备处理非线性关系 . 

问题在于，互信息原本是用来计算离散变量的相关性指标，但是我们的数据常常是连续的数值. 

MIC将关系视为分布，人为地将连续数据变为离散数据. 它将X、Y的散点图绘制在一个x*y的网格上. 它并不关心划分网格后，每个格子内的散点呈现什么趋势，而是关心这个格子内是否有点、有多少个点. 也就是说，利用二维网格去统计网格内的概率分布P(X,Y)，这样不管变量X、Y是周期还是函数关系都可以近似其分布，从而反映是否呈现某种趋势. 


MIC定义为在不同分辨率的网格下，归一化互信息的最大值：
$$\text{MIC}(D) = \max_{xy < B(n)} \{ M(D)_{x,y} \}$$


## 生成矩阵的近似算法

```bash
网格有无限多画法，怎么快速找到最优解
```

计算MIC需要找到给定列数x和行数y下的最优网格划分. 由于网格的划分为之有无穷多种，因此这是一个NP-Hard问题. 原论文通过动态规划实现了近似的最优解. 

我们将原问题简化为固定Y，将Y划分为若干个区间、去寻找X的最优划分. 这么做的目的是将原问题转化为如何通过切分X轴，以将Y轴上的熵降低. 说得再简单些，就是寻找一种网络切分数据点，让数据点看上去最不随机. 

虽然规定固定了在Y轴上的划分，对X轴仍然需要进行遍历式的切分，只不过我们的目的是找出能得出最优解的i，因此我们只需要对<m的i进行遍历就行了，只不过这个过程中有很多个计算结果是可以复用的. 

对于X轴的划分是本算法的核心. 对于X轴的划分过程遵循动态规划算法的迭代. 作者设计了一个目标函数：

![公式1](/illustrations/dongtaiguihuagongshi.png )

这个公式本质上是基于信息论以及前文所提及的MIC计算方式所建立的. 笔者在这里进行简单的推导：

根据互信息定义 $I(X; Y) = H(X) - H(X | Y) = H(Y) - H(Y | X)$，由于数据集固定，H(Y)是一个常数，因此最大化互信息等价于最小化条件熵. 

定义信息量：
$$\log \frac{1}{p(x)} = -\log p(x)，$$

则熵
$$H(x) = \sum p(x) \cdot \log \frac{1}{p(x)} = -\sum p(x) \log p(x)；$$

联合熵
$$H(x,y) = -\sum p(x,y) \log p(x,y). $$

对于划分后的每一列内部熵的加权和 $H(Y|X) = \sum_{j=1}^{L} \frac{N_j}{N} H(Y_j)$ ，定义我们所需要的最大化目标函数F（即负的条件熵）：

$$F = -H(Y|X)；$$

展开，分为前L-1列和第L列：

$$F = \sum_{j=1}^{L-1} - \frac{N_j}{N} H(Y_j) + \left( - \frac{N_L}{N} H(Y_L) \right)$$

最终递归形式写为：

$$\frac{i}{m} F(i, l-1) - \frac{m-i}{m} H(Y_L). $$

简单来说，虽然I(X,Y)通常写为整体形式，但是在计算的时候可以分解为每一个列对整体熵的贡献，从而建立起状态转移方程F. MIC的本质过程是寻找一种切分X轴的方式使得每个网格内布的分布尽可能地纯净，也就是说我们需要去减少列内熵H(Y|X)，因为总熵是固定的，减少了列内熵，就相当于增大了互信息. 而F的核心思想是，当前l列的最优解取决于前l-1列的最优解加上l列新增的信息量. 

## 不可或缺的过程优化

在计算F之前，须要对X轴进行压缩，否则计算量仍然相当大. 压缩的方法也很简单，整个过程是一种行程编码，在切分Y轴之后，为每一个数据点标记行号ROW ID，然后从左往右扫描数据点，若当前点和上一个点的Y轴处于同一个ROWID，则把他们放在同一个块里. 如果ROW ID发生了改变，那说明发生了断裂，自行形成一个新的块. 

换言之，我们新生成了一个边界，这个边界在不破坏原有分布的情况下缩小了计算的开支，前提是D已经是按X轴划分好的. 判断边界的确需要耗费算力，但是这个开销是线性的，但是计算动态规划算法的复杂度却是平方甚至立方的. 

---

此外，在面临个很多拥有同样Y值的点时，应当将Y轴分为y行，每行包含大致相同的点. 具体来说，先把数据按Y值从小到大排好队，然后找出当前所有Y值相同的点，视为同一个集合S；然后决定：将S塞进当前行，还是开启下一行？

评判标准是，塞进去之后距离完美目标差距多少？不塞进去，当前行的点数距离完美目标差多少？如果塞进去之后导致误差增大，那么久换行. （currROW + 1, line9）决定收尾，则在S中给每一个点都打上标签Q(p) = currROW. 
    
    例如，目标是每行6个人，第一行已经装了3个考78分的人，那么接下来连续4个考91分的人是强塞到第一行，还是换行？
    如果强塞，第一行就变为3+4=7人；误差=|7-6|=1;
    如果换行，第二行就变为3人，误差=|3-6|=3. 
    
*如果误差一样，通常就塞进去. *

为什么要这么做呢？按理来说，Y的分布越聚焦，我们就越兴奋——说明某个区间的概率越大，为什么非得抹平呢？

需要注意的是，Y自身的特征并不意味着X和Y的关系特征. 我们强制地将Y轴划分为每行包含大致相同数量的点，并非抹除X和Y的关系，反而是最大化我们发现关系的能力. 

究其数学原理，是因为**互信息I(X; Y)永远不可能超过变量自身的熵H(Y),即I(X;Y) <= H(Y)**.

而当一个变量的分布是均匀分布时，它的熵是最大的. 如果我们瞎切，极有可能某些地方行点多，有的地方行点少，那么H(Y)就会变小. 因此强制Y轴均分就是为了让H(Y)达到最大值. 如果这个时候算出来I还是很小，说明X和Y没有什么关系；但是如果此时I算出来很大，那说明是真有关联. 

```bash
举个例子说明：
癌症患者最终痊愈的人很少，死去的人极多. 如果想要调查痊愈Y和某个事件例如某种药物X的关联，画一条线，上面几乎都是痊愈（点很少），下面是死亡（点很多），如果强制均分，那么在吃药同时，可能有98%的情况都是病人死去. 
```

同时，作者也设计了一种动态找补机制. 如果第一行确实最终只装了3个人，那么欠的2个名额先记下，剩下的3个名额会均分给下一个行. 

---

以下内容是关于公平性的证明. 

## 证明：MIC->0当且仅当X和Y统计独立

首先需要确定以下引理：

引理1：

$$\Pr \left[ \left| \sum X_i - pn \right| \geq \varepsilon pn \right] \leq 2^{-\Omega(pn\varepsilon^2)}$$

这个公式是乘法切诺夫界，回答的是实际发生的次数（样本）偏离期望值（理论）的概率有多大. 其中 $X_1, \dots, X_n$可以视为n次的硬币投掷，每一次成功的概率是p，失败的概率是1-p，$\sum X_i$ 是n次实验中成功的实际次数，pn 则是数学期望. $\left| \sum X_i - pn \right| \geq \varepsilon pn$ 描述的是极端情况，即实际成功次数偏离预期值的程度，超过了预期值的 $\varepsilon$ 倍（ $\varepsilon$ 是一个很小的比率，例如10%）. $\Pr[\dots]$ 表示发生这种极端事件的概率. 

整个公式的结果 $\leq 2^{-\Omega(pn\varepsilon^2)}$ ，意思是说这种偏离预期的理论概率可能性基地，它保证了大样本下的观测值是高度可信的. 

---

引理2：

![公式2](/illustrations/shuxuegongshiofmic.png )

$I(D|_G)$是根据样本点在网格G中实际算出来的互信息，$I(X; Y)$是该概率分布在网格上的理论真实互信息. 它们的差值即观测值与真实值的差异. 

$\frac{B}{2n}$ 中的B是网格总数，n为样本量，意为网格越密集（B越大）或样本越少，误差越大. 

$\varepsilon_{i,j}$ 即误差项，表示在第i行第j列网格中，样本点的实际比例与理论概率之间的微小差值. 根据引理1，这个误差会随着样本量的增加而迅速变小. 

这个公式的推导引用了Roulston(1999)的论文[50],实际上它是基于泰勒展开的. 

熵定义为 $H(p)=-\sum p_i \log p_i$ ，假设真实的概率是p，我们观测到的频率是 $\hat{p} = p + \epsilon$ ，我们需要计算 $\hat{p} \log \hat{p}$ . 

在p附近对函数 $f(x) = x \log x$ 进行二阶泰勒展开：

$$(p + \epsilon) \log(p + \epsilon) \approx p \log p + (1 + \log p)\epsilon + \frac{1}{2p} \epsilon^2$$

带入互信息公式：

$$I = \sum p_{xy} \log \frac{p_{xy}}{p_x p_y}$$ 

将带有误差的 $\hat{p}$ 代入展开式，经过代数运算（很多项会抵消，因为 $\sum \epsilon=0$ ），最后留下的主导项就是 $\frac{1}{2n} \times (\text{自由度})$ 

对于一个x * y大小的网络，自由度大约就是格子数B. 所以误差的主导项是B/2n，也就是Miller-Madow偏差矫正的来源. 

---

引理3：

$$I(D) = H^Y(D) - \sum_{j=1}^x p_X(j)H^Y_j(D)$$

其中 $H^Y(D)$ ：边缘熵，即只看行（Y轴）时的信息不确定性；

 $p_X(j)H^Y_j(D)$是 条件熵的组成部分. 指当我们在X轴的第j列时，Y轴方向上点分布的不确定性，并按该列的点数比例加权. 因此总的来看，这个公式等价于将原互信息的计算方式等价地转变为：网格互信息等于“总体的行不确定性”减去“给定列之后的平均行不确定性”. 

---

定义4：(m, α) -partitionable. 指一组点是否可以被分成最多 $m$ 份，使得这份“离散化”后的信息熵至少达到理论最大值的 $\alpha$ 倍. 

---

定义5：m-equipartitionable：指点集可以被完美地“等分”（每个格子点数一样）

---

引理6. 6
对于任何足够大的样本集n，总能找到一种划分方式，使得它非常接近于完美的等分状态. 

---

这些定义和引理证明了：即便数据点分布很随机，随着样本量n增加，我们总能通过调整网格线，找到一种接近“等分”的划分方式. 

---

### 证明思路

1.  如果X和Y统计相关，那么MIC>0；
2.  如果X和Y统计独立，那么MIC=0. 

对于相关数据，利用大数定律保证样本分布收敛于真实分布；

对于独立数据，利用组合数学和切诺夫界证明噪音无法伪装为信号. 

### 证明：X和Y相关 => MIC > 0

两个变量独立的定义是它们的累积分布函数可以分解：
$$F(x, y) = F(x) \cdot F(y)$$

也就是说对于任意的a，b有：

$$P(X<a, Y<b) = P(X<a) · P(Y<b)$$

以上的公式的逆否问题为：当X、Y不独立，那么上述等式至少在某一个点(a,b)处不成立. 也就是说，存在某一对(a,b)，使得在该点 $P(X < a, Y < b) \neq P(X < a) \cdot P(Y < b)$. 

那么我们就在这个特定的点(a,b)处画一个十字架，把平面切分为四个象限，那么这4个象限的概率分布就是网格分布$D|_G$，由于在(a,b)处不独立，因此这个表格的联合概率不等于边缘概率的乘积. 

也就是说，只要相关，就至少存在一个2×2的切分，能够捕捉到这种相关性，记为结论6. 7. 

当我们有n个样本点时，我们用同样的a和b去划分这n个点，根据大数定律，随着n趋于无穷大，那么落在这四个象限的样本比例会无限接近于真实频率. 

互信息I是一个关于概率的连续函数，既然样本比例（频率）趋近于真实概率，那么杨根在这个2*2网格上的互信息I也会趋近于理论I，又因为理论I>0，因此对于足够大的n，样本I也会大于某个正数. 

回头再去看MIC的定义：

$$MIC(D) = \max_{\text{all grids}} \{ \text{Normalized MI} \}$$

MIC搜索所有可能的网格，并取最大值. 我们并不需要知道MIC最终找到了哪个复杂的网格，只需要知道，既然我们刚才构造的哪个简单的2*2网格得分都>0了，那么MIC也一定>0. 

MIC公式里还有一部分归一化，最后除以了 $\log(\min(x,y))$ ，对于2*2的网络，分母是 $\log(2)=1$ ，所以归一化不会把分数变为0. 

### 证明：对于统计独立的数据，MIC收敛于0

对于所有的网格，只要样本量足够大，它们算出来的互信息在独立数据熵都趋近于0.  这个证明略有难度，我们需要把概率统计问题变成排列组合问题. 

在独立的情况下，X和Y没有关系，这意味着我们把X轴的数据从小到大排好序，那么对应的Y轴数据就是一堆乱序的数据. 

证明的思路是：
1.  将连续变量的独立性问题转化为离散的随机排列问题；
2.  证明对于一个随机排列，无论怎样网格划分，落入每个点的数都非常接近平均值（即分布是均与的）
3.  分布是均匀时，根据互信息的定义，其互信息为0，那么MIC->0. 

MIC是基于秩统计量的. 它并不关心具体X、Y等于多少，而是关心这是第几个X、第几个Y. 既然X、Y相互独立，那么不管X怎么排序，Y的取值都是完全随机的（结论6. 9）. 

有必要说明的是，这里的秩是统计而非线性代数中的概念. 这里的秩指的是一个数值在一组数据中按大小排序后的位置. 

由此可以假设，X的秩是有序序列{1, 2, 3. . . }，则Y的秩是从{1, 2, 3. . . }中均与随机抽取的一个序列，记为 $\sigma$ . 

想象我们现在绘制了一个 $x \times y$ 的网络，由于X是有序的，那么根据定理6. 9，Y也是乱序的. 打个比方，随便往一个棋盘上丢一堆棋子，只要棋子足够多，那么每个格子里的棋子数量按理来说是差不多的，不会出现左上角特别少、右下角特别多的情况. 

设网格有k个格子，样本有n个，每个格子里的期望点数是n/k. 根据引理6. 1，对于随机排列，某个格子中的点数偏离期望值很大的概率呈指数级衰减. 又因为格子总数K远小于 $2^{\Omega(\log^2n)}$  ，因此即便把所有格子加起来，出错的概率仍然趋近于0.  

由上可得出结论，对于一个随机排列，如果我们画一个网格（ $ky \ll n$ ）,那么几乎必然地，每个格子里的是几点疏都非常接近期望值. 
具体公式为：

$$|\epsilon_{i,j}| \le \frac{\log n}{\sqrt{kyn}}$$

当n增大时，这个误差会趋近于0. 

当然，MIC并不是一个固定网络，而是用动态规划的思想去搜索. 但是可以证明的是，在随机排列中，出现长传连续相同的团块概率很低，即便算法试图利用这些微小的团块，但是因为它们实在过少和太小，对整体I的贡献微乎其微. 

回忆6. 2所提及的公式，$I_{true} = 0$，因为X和Y是独立的. 因此我们需要证明 $I(D|_G) \approx O(\sum \epsilon^3) \to 0$ . 

前面我们已经证明，在随机排列下，误差被限制在 $\frac{\log n}{\sqrt{n}}$ 这个级别. 将  $\epsilon$  的界代入互信息的误差公式中，误差项大约是 $k \times (\frac{\log n}{\sqrt{n}})^2 \approx \frac{k \log^2 n}{n}$. 

此外，MIC定义中，有一条限制为$B(n) = O(n^{1-\epsilon})$ . 设置这个限制的理由在于，由于总格子数 $k \le B(n)$，如果B(n)增长得比n慢，那么：

$$\text{Error} \approx \frac{n^{0. 6}}{n} = n^{-0. 4}$$

当 $n \to \infty$ 时，n的0.4次方趋于0 . 

由此得出结论，因为网格划分得不够细（受限于B(n)），而数据量n又增长得很快，所有的随机波动（噪音）都会被平滑掉，最终计算出的互信息 $I(D|_G)$ 只能收敛于 0. 

这个设限实际上是一种很聪明的防守. 对于一组完全随机独立的噪音，我可以画一个足够细的网格，使得每个数据点都独立地落在一个格子里，而且这个格子所在的行和列都没有其它点，在这种情况下，一道我知道了某个点在哪一列，就知道它在哪一行. 这意味着条件熵等于0，算出来的MIC会接近1. 用机器学习中常用的话术来说，就是出现了过拟合. 


## 证明：对于绝大多数无噪函数，MIC趋近1

证明思路为：

1.  建立通用思路，证明如果能将数据隔离开来，就能取得很高的MIC分数；
2.  将这个工具应用到f(X)上，计算格子是否够用；
3.  取极限；

令D为n个数据点，对x轴进行连续变量采样，此时它们的x值都是唯一的. 假设到我们已经把Y轴切分为了y行，而且切分是均匀的，这意味着每一行里的点数大概是n/y. 

令z为跳跃次数，当我们沿着x轴从左往右看这n个点时，y值所在的行号发生了多少次变化；
如果点p1在第一行，p2也在第一行，没变；如果p1在第一行，p2在第三行，这就是一次跳跃. z就是总共换行的次数. 

我们需要构造一个特定的网格G，对Y轴进行均匀划分，对X轴只在跳跃的地方画竖线. 

因为只有z次跳跃，所以我们画了z条竖线，把平面分成了z+1列，总格子数为：

$$N_{cells} = \text{行} \times \text{列} = y(z+1)$$

沿用定理6. 3的公式。因为在Y轴上是均匀划分，每行的概率都是1/y，均匀分布的熵最大，为 $\log y$ . 

在我们的构造中，我们把同一行的连续点都包在了一列里，一旦换行就需要切断，那么在任意一列中，所有数据都在同一行，那么列内部的概率分布就是100%在某一行，0%在其他行，这种确定性分布的熵=0.

这样的结果就是，
$$I(D|_G) = \log y - 0 = \log y$$

带入归一化的MIC公式：
$$MIC \approx \frac{I(D|_G)}{\log(\min(y, z + 1))}$$

由于我们通常让 $y$ 和 $z$ 处于同一量级，分母大约是 $\log y$. 那么此时，
$$MIC≈\frac{\log y}{\log y} = 1 . $$

结论就是，只要B(n)足够覆盖y(z+1)个格子，那么MIC分数就为1. 

对于一般的函数f(x), y(z+1)确实小于B(n), 因为通常设定的网格总数B(n)= $n^{0. 6}$ . 设定y轴的行数 $y = B(n)^{\alpha}$，其中 $\alpha < 1/2$ . 例如如果 $B(n)=100$，我们可以设 $y=10$ （即 $\alpha=0. 5$ ）. 

对于一个处处不为常数的函数f，它的几何性质决定了如果画y条水平线，这条函数曲线穿过这些线的次数是有限的，且与y成正比. 令k为某个常数（取决于函数的波动程度），则z<=ky. 

*如果函数震荡极其剧烈例如 $y=\sin(1/x)$ ，这个k可能就是无穷大了，但是这里讨论的是良好性质的函数. *

所需格子数≈y · x = y · ky = k · y^2, 带入 $y = B(n)^\alpha$ ：

$$\text{所需格子数} \approx k \cdot (B(n)^\alpha)^2 = k \cdot B(n)^{2\alpha}$$

又因为我们设定了 $\alpha < 1/2$ ，所以 $2\alpha < 1$ . 当n足够大时，B(n)也足够大，一个较大数的2\alpha$ 次方（比如 $0. 9$ 次方）肯定远小于它的一次方. 即: $k \cdot B(n)^{2\alpha} \ll B(n)$ . 

因此我们完全有能力画出理想的网格. 

前面的推导假设 Y 轴可以完美均分（熵为 $\log y$ ）. 实际上，因为 $n$ 是离散的，可能无法整除 $y$ ，导致分布有微小的不均匀. 根据结论6. 6，这种不均匀导致的熵的损失只有 $\epsilon$ ，即 $H^Y(D) \ge (1-\epsilon) \log y$ . 

带入MIC公式：

$$MIC(D) \geq \frac{H^Y(D)}{\log(\min\{y, z + 1\})}$$

其中，分子: $\ge (1-\epsilon) \log y$ 

分母：由于 $z \approx ky$ ，所以 $\log(z+1) \approx \log(ky) = \log k + \log y$. 当 $n \to \infty$ 时， $y \to \infty$ ，常数 $\log k$ 可以忽略. 所以分母 $\approx \log y$ . 

此时取极限：
$$MIC(D) \geq \lim_{n \to \infty} \frac{(1 - \epsilon) \log y}{\log y} = 1 - \epsilon$$

因为 $\epsilon$ 可以任意小（只要 $n$ 够大），所以MIC(D)->1, 证毕. 

## 证明： 对于大多数由不同可谓曲线构成的有限并集，MIC趋近于1

面对的是一个复杂的几何形状（比如一个圆 $x^2 + y^2 = 1$）时，一个X可能对应不止1个Y. 此时的策略是把形状拆解为函数的集合. 

假设数据分布在有限个光滑曲线的病机熵，根据隐函数定理，任何满足上述条件的曲线都可以被切分为有限段，每一段都可以看为一个关于X的函数 $f_i(x)$ . 

原本的几何问题变成了：数据落在集合 $\mathcal{F} = {f_1, \dots, f_m}$ 的图像上，证明如果数据落在有限个函数 ${f_1, \dots, f_m}$ 的并集上，MIC $\to$ 1 . 

依然设定行数 $y = B(n)^\alpha$（其中 $\alpha < 1/2$），并保证Y轴是均匀划分以达到最大熵. 问题在于，我们有m个函数，因此我们在任意一个函数跨越行界限时，画一条竖线. 

仍然令 $z_i$ 为第 $i$ 个函数 $f_i$ 跨越行界线的次数. 对于光滑函数，z与行数y成正比，总的竖线量 $Z = \sum_{i=1}^m z_i \approx m \cdot c \cdot y$ . 

网格总数 $Total = x \cdot y \approx (mcy) \cdot y \propto y^2$ ，因为 $y = B(n)^\alpha$ 且 $\alpha < 1/2$，所以 $y^2 = B(n)^{2\alpha} \ll B(n)$. 因此当n足够大时，我们仍然可以划分出这样的网络. 

在多个函数中，给定一列X，数据可能落在不同个格子里，例如对于圆形，一列可能切中上下两个点. 

那么对于互信息公式中的H(Y|X)，非空的格子最多只有m个（因为只有m个函数），哪怕数据点在这m个格子里均匀分布，其熵的最大值也只是 $\log m$. m是函数个数，不随样本量增加，因此 $I(D|_G) \geq \log y - \log m$ . 

计算MIC：
$$MIC \approx \frac{I(D|_G)}{\log(\min\{x, y\})}$$

由于 $x$ 和 $y$ 是同一量级（都取决于 $B(n)^\alpha$），根据之前的证明，分母大约是 $\log y$. 展开：

$$MIC \geq \frac{\log y - \log m}{\log y} = 1 - \frac{\log m}{\log y}$$

当样本量n趋于无穷大时，网络行数 $y = B(n)^\alpha \to \infty$，但是函数个数是常数，因此，

$$\lim_{n \to \infty} \left( 1 - \frac{\text{常数}}{\infty} \right) = 1 . $$

这个证明实际上说明，当数据量极大时，MIC会使用非常细的网格，数据的整体结构也就是H(Y)会随着网格辨析，信息量以log y级别增长，而虽然一个X对应多个Y，导致我们无法完美预测，但是这个不确定性log m是固定的. 


---

### 抗噪能力

当数据混入噪音时，一个重要的问题是，MIC的分数到底代表了多少噪音？在统计学中，决定系数是衡量线性关系的强度，MIC的作者也同样证明，MIC的得分有一个下界并且与决定系数直接相关. 更严谨地说，随着噪音的增加，MIC也会下降，而且MIC的值通常不低于该数据的决定系数. 

这个性质的好处在于，它赋予了MIC具体的物理含义，如果算出来MIC=0. 8，可以粗略地将结果的可解释性与决定系数通用和易懂. 而且对于MIC最重要的公平性而言，MIC在差不多的噪音水平（决定系数）下给直线、正弦波、指数函数的分数应该都差不多，这是对公平性最好的证明，它说明了MIC是一个好用的度量. 

## 什么是更好的统计量

评判某个统计量足够好，往往从普适性（值为0当且仅当变量独立）、公平性（对于含噪程度相同的不同关系，统计量应当给出相同的分数）、统计功效（样本量很小或信号微弱时，能够以多大的概率拒绝独立的原假设）、计算效率和维度扩展（计算速度块且能处理高维向量）四个方向出发. 

MIC在前两个标准上是比较好的，因为它能够捕捉非线性的关系，而且公平性良好，并且能用于连续的数据. 

不过统计量的集合中仍然有很多种方案. 例如距离相关系数、TIC、核方法、MINE. MIC的原作者团队也承认MIC存在缺陷，并且设计了一种改进的方案TIC，将取最大值修改为了求改良后的总信息量. 在深度学习处理高维、大规模图像或文本数据时，MINE表现出非常优异的性能. 

总地来说，根据场景需求，选择不同的统计量是必要的. 并非所有的数据都叫大数据，当数据量足够小时，dCor能不遗漏各种蛛丝马迹；探索性分析的场景里，TIC会有更加精准的表现；当要求计算效率、可微、并行时，又会有其它的function. 